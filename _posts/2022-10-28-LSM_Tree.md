---
title: Log-Structured-Merge-Tree
description: 代码整洁之道读书笔记
layout: keynote
author: 陈家辉
tags:
- 数据结构
- 物理存储
---
## 简介

LSM树即 **Log-Structured-Merge-Tree**， 日志结构合并树。它是Google发表的论文 *Big Table* 中提到的一种很有趣的文件组织数据结构， 现如今已经被运用在很多工业界的产品之中了：HBase、Cassandra、LevelDB、RocksDB等等。相较于B树，它拥有更优秀的写性能。

## 基本思想

众所周知，磁盘IO分为随机读写和顺序读写，顺序读写的速度远远快于随机读写，以获取更高的IO速率。LSM树会将所有的随机写操作转换为顺序写操作。

## 组成

![img](https://cdn.jsdelivr.net/gh/Chenjiahui0/picture@main/202306231018977)

LSM由三个部分组成

### MemTable

顾名思义，memtable是在内存中的数据结构，用以保存最近的一些更新操作，当写数据到memtable中时，会先通过WAL的方式备份到磁盘中，以防数据因为内存掉电而丢失。

memtable可以使用跳跃表或者搜索树等数据结构来组织数据以保持数据的有序性。当memtable达到一定的数据量后，memtable会转化成为immutable memtable，同时会创建一个新的memtable来处理新的数据。

### immutable memtable

顾名思义，immutable memtable在内存中是不可修改的数据结构，它是将memtable转变为SSTable的一种中间状态。目的是为了在转存过程中不阻塞写操作。写操作可以由新的memtable处理，而不用因为锁住memtable而等待。

### SSTable

SSTable(Sorted String Table)即为有序键值对集合，是LSM树组在磁盘中的数据的结构。如果SSTable比较大的时候，还可以根据键的值建立一个索引来加速SSTable的查询。下图是一个简单的SSTable结构示意：

![img](https://cdn.jsdelivr.net/gh/CJH876492153/picture@main/fig3-5.png)

因此当MemTable达到一定大小flush到持久化存储变成SSTable后，在不同的SSTable中，可能存在相同Key的记录，当然最新的那条记录才是准确的。这样设计的虽然大大提高了写性能，但同时也会带来一些问题：

1. 冗余存储，对于某个key，实际上除了最新的那条记录外，其他的记录都是冗余无用的，但是仍然占用了存储空间。因此需要进行Compact操作(合并多个SSTable)来清除冗余的记录。
2. 读取时需要从最新的倒着查询，直到找到某个key的记录。最坏情况需要查询完所有的SSTable，这里可以通过前面提到的索引/布隆过滤器来优化查找速度。

## CRUD

### 写入

写操作首先需要通过WAL将数据写入到磁盘Log中，防止数据丢失，然后数据会被写入到内存的memtable中，这样一次写操作即已经完成了，只需要1次磁盘IO，再加1次内存操作。相较于B+树的多次磁盘随机IO，大大提高了效率。随后这些在memtable中的数据会被批量的合并到磁盘中的SSTable当中，将随机写变为了顺序写。

### 删除

在memtable中插入一条数据当作标志，如`delKey:1933`，当读操作读到memtable中的这个标志时，就会知道这个key已被删除。随后在日志合并中，这条被删除的数据会在合并的过程中一起被删除。

### 更新

更新和写入是一样的，直接写入memtable中。如果在memtable中有该key就会覆盖，如果不在就直接写入memtable。

### 查询

读操作需要依次读取memtable、immutable memtable、SSTable0、SSTable1......。需要反序地遍历所有的集合，又因为写入顺序和合并顺序的缘故，序号小的集合中的数据一定会比序号大的集合中的数据新。所以在这个反序遍历的过程中一旦匹配到了要读取的数据，那么一定是最新的数据，只要返回该数据即可。但是如果一个数据的确不在所有的数据集合中，则会白白得遍历一遍。

## Compact

由于将内存数据合并到磁盘当中会产生大量的小的集合，并且更新和删除操作会产生大量的冗余数据，通过合并操作可以减少集合中的冗余数据并降低读操作时线性扫描的耗时。

![img](https://cdn.jsdelivr.net/gh/Chenjiahui0/picture@main/202306231018978.png)

### 策略

目前广泛使用的有两种合并策略，size-tiered策略和leveled策略

不过在介绍这两种策略之前，先介绍三个比较重要的概念，事实上不同的策略就是围绕这三个概念之间做出权衡和取舍。

1. 读放大:读取数据时实际读取的数据量大于真正的数据量。例如在LSM树中需要先在MemTable查看当前key是否存在，不存在继续从SSTable中寻找。
2. 写放大:写入数据时实际写入的数据量大于真正的数据量。例如在LSM树中写入时可能触发Compact操作，导致实际写入的数据量远大于该key的数据量。
3. 空间放大:数据实际占用的磁盘空间比数据的真正大小更多。上面提到的冗余存储，对于一个key来说，只有最新的那条记录是有效的，而之前的记录都是可以被清理回收的。

#### size-tiered

![img](https://cdn.jsdelivr.net/gh/Chenjiahui0/picture@main/202306231018979)

每层的SSTable个数固定为N，但是每个SSTable大小不作限制。当某层个数超过N时，会进行compact，并将其放置到下一层。

*当层数达到一定数量时，最底层的单个SSTable的大小会变得非常大*。并且*size-tiered策略会导致空间放大比较严重*。即使对于同一层的SSTable，每个key的记录是可能存在多份的，只有当该层的SSTable执行compact操作才会消除这些key的冗余记录。

这种策略有一个缺点是当集合达到一定的数据量后，合并操作会变得十分的耗时。

**有较低的写放大，但是读放大和空间放大较高。**

#### leveled

![img](https://cdn.jsdelivr.net/gh/Chenjiahui0/picture@main/202306231018980)

每层SSTable数量逐渐增加，每一层限制总文件的大小。

leveled会将每一层切分成多个大小相近的SSTable。这些SSTable是这一层是**全局有序**的，意味着一个key在每一层至多只有1条记录，不存在冗余记录。之所以可以保证全局有序，是因为合并策略和size-tiered不同，接下来会详细提到。

![img](https://cdn.jsdelivr.net/gh/Chenjiahui0/picture@main/202306231018981.png)

rocksDB采用混合压缩策略，在L0层采用size-tiered compaction，当L0层文件数目达到规定值后就会触发L0合并到L1。

![img](https://cdn.jsdelivr.net/gh/Chenjiahui0/picture@main/202306231018982.png)

当level1到达压缩的阈值， L1 中选择至少一个文件，并将其与 L2 的重叠范围合并。

![img](https://cdn.jsdelivr.net/gh/Chenjiahui0/picture@main/202306231018983.png)

当然，也可以并行压缩多个SST

![img](https://cdn.jsdelivr.net/gh/Chenjiahui0/picture@main/202306231018984.png)

### 二者的比较

一个例子是以恒定的速度直接写入新数据，我们看到采用STCS压缩期间临时磁盘空间使用率很高——在某些时候需要的磁盘空间量增加了一倍。而LCS相对稳定。在写入的过程中，STCS会有严重的空间放大

![img](https://cdn.jsdelivr.net/gh/Chenjiahui0/picture@main/202306231018985.png)

第二个示例是覆盖工作负载，其中相同的 1.2 GB 数据集被反复写入 15 次。

通过 size-tiered compaction，我们看到了巨大的空间放大——在运行期间多次需要磁盘上多达 9.3 GB 的空间（几乎是 8 倍的空间放大）。

使用水平压缩，空间放大要低得多，如下图所示。我们看到空间放大实际上达到了超过预期的 1.1 — 2倍，因为我们在压缩之前等待 L0 中采用Size-tired compaction收集 4 个 sstables，在此设置中，4 个 L0 sstables 大约是 400 MB 的数据，这也是所有重复数据。

![img](https://cdn.jsdelivr.net/gh/Chenjiahui0/picture@main/202306231018986.png)

从空间放大的角度来看LCS比STCS表现好得多，然而LCS有一个更为严重的问题，那就是写放大。写放大意味着系统的I/O工作负载更大，这样会直接影响到查询的性能。

#### Space Amplification Goal

LCS的优化，尽量平衡写放大。

## 优化

从上面的描述中可以看出，LSM的写性能十分优越，然后读的性能却很一般，时间复杂度为O(N)。

对于读而言，主流有两个优化方案：

1. BloomFIlter
2. Cache

使用布隆过滤器可以隔绝掉绝大部分不存在的key请求，避免空请求耗费大量的查询时间。

Cache对于高频热点数据进行缓存，节省查询开销。



## Benchmarks

10,000,000 operations using 8 byte keys and 96 byte values

Random writes:

| Software             | Time        |
| -------------------- | ----------- |
| Indeed LSM Tree      | 272 seconds |
| Google LevelDB       | 375 seconds |
| Kyoto Cabinet B-Tree | 375 seconds |

Random Reads:

| Software             | Time        |
| -------------------- | ----------- |
| Indeed LSM Tree      | 46 seconds  |
| Google LevelDB       | 80 seconds  |
| Kyoto Cabinet B-Tree | 183 seconds |

Same benchmark using cgroups to limit page cache to 512 MB

Random Reads:

| Software             | Time        |
| -------------------- | ----------- |
| Indeed LSM Tree      | 454 seconds |
| Google LevelDB       | 464 seconds |
| Kyoto Cabinet B-Tree | 50 hours    |

## CODE

## 引用

[LSM树详解](https://zhuanlan.zhihu.com/p/181498475)

[LSM树原理探究](https://juejin.cn/post/6844903863758094343)

[LSM Compaction Strategy](https://www.jianshu.com/p/8de55d5df05e)

[纯干货！深入探讨 LSM Compaction 机制](https://zhuanlan.zhihu.com/p/140325974)

[facebook-rocksdb](https://github.com/facebook/rocksdb/wiki/Leveled-Compaction)

数据密集型应用设计-第三章

[Scylla’s Compaction Strategies Series: Write Amplification in Leveled Compaction](https://www.scylladb.com/2018/01/31/compaction-series-leveled-compaction/)

[Incremental Compaction 2.0: A Revolutionary Space and Write Optimized Compaction Strategy](https://www.scylladb.com/2021/04/28/incremental-compaction-2-0-a-revolutionary-space-and-write-optimized-compaction-strategy/)

## 文献

[The Log-Structured Merge-Tree (LSM-Tree)](https://www.cs.umb.edu/~poneil/lsmtree.pdf)

[The Design and Implementation of a Log-Structured File System](https://web.stanford.edu/~ouster/cgi-bin/papers/lfs.pdf)