---
title: etcd入门篇二之客户端设计
layout: post
header-img: "https://w.wallhaven.cc/full/ex/wallhaven-ex9gwo.png"
description: etcd系列第二篇博客
author: 陈家辉
tags:
- etcd
---

# 介绍

etcd绝大部分复杂的应用逻辑由服务端完成，并且服务端可以保证服务的健壮性。虽然服务器组件是正确的，但它与客户端的组合需要一套不同的复杂协议来保证其正确性和故障情况下的高可用性。理想情况下，etcd 服务器提供许多物理机器的一个逻辑集群视图，客户机实现副本之间的自动故障转移。

## 专有词汇

*clientv3*：基于etcd v3 API 的官方Go语言客户端。

*均衡器*：etcd客户端负载均衡器实现重试和故障转移机制。 etcd 客户端自动平衡多个端点之间的负载。

*端点*“： 客户端可以连接到的 etcd 服务器端点列表。通常，一个 etcd 集群的3个或5个客户端 URL。

*固定端点*： 当配置有多个端点时，< = v3.3客户端均衡器只选择一个端点来建立 TCP 连接，以保存到 etcd 集群的总开放连接。在 v3.4中，均衡器轮循为每个请求固定端点，从而更均匀地分配负载。

*客户端连接*：通过 gRPC Dial 建立到 etcd 服务器的 TCP 连接。

*子连接*: gRPC SubConn 接口。每个子连接包含一个地址列表。Balancer 根据已解析地址列表创建 SubConn。GRPC ClientConn 可以映射到多个 SubConn (例如 example.com 解析为两个子连接中的10.10.10.1和10.10.10.2)。Etcd v3.4均衡器使用内部解析器为每个端点建立一个子连接。

*临时断联*： 当 gRPC 服务器返回代码状态错误时，不可用。

# 需求

**正确性**

如果存在服务器故障，请求可能会失败。然而，一定不会违背一致性保证：全局顺序性，不写错误数据，可变操作最多一次语义，局部事件不会被观测等等。

**存活**

服务端可能故障或短暂失联。客户端不应该死锁等待服务器从脱机状态恢复，除非配置为这样做。理想情况下，客户端通过 HTTP/2 ping 检测不可用的服务器，并通过明确的错误消息将故障转移到其他节点。

**有效性**

客户端应该以最少的资源有效地操作: 以前的 TCP 连接应该在端点切换之后优雅地关闭。故障转移机制应该能够有效地预测下一个要连接的副本，而不会在失败的节点上浪费时间重试。

**便携性**

正式客户端应该有明确的文档，其实现应该适用于其他语言绑定。不同语言绑定之间的错误处理应该是一致的。由于 etcd 完全致力于 gRPC，因此实现应该与 gRPC 长期设计目标密切一致(例如，可插入重试策略应该与 gRPC 重试兼容)。客户端版本的升级应该是无干扰的。

# 概述

客户端的由以下组件组成：

* 均衡器，用于与服务端集群建立gRPC链接
* API客户端负责发送RPCs给服务端，以及错误处理，它决定是否重试失败请求或者切换端点。

语言可能在建立一个初始化链接上（例如配置TLS）、编码和传输Protobuf信息给服务端，怎么样去处理RPCs流之类的细节上有区别。但是etcd服务端返回的错误都是一致的，客户端的错误处理和重试策略也应当如此。

接下来会介绍官方提供的各版本客户端均衡器的实现与局限性。

## clientv3-grpc1.0

### 均衡器概览

当配置了多个etcd端点时，`clientv3-grpc1.0`会保持多个TCP链接。然后挑选一个端点发送所有的客户端请求。

![client-balancer-figure-01.png](https://cdn.jsdelivr.net/gh/CJH876492153/picture@main/client-balancer-figure-01.png)

### 均衡器局限性

打开多个 TCP 连接可能提供更快的均衡器故障转移，但需要更多资源。均衡器不了解节点的健康状态或集群成员身份。因此，均衡器有可能在一个失败或网络分区的节点上卡住。

## clientv3-grpc1.7

### 均衡器概览

`clientv3-grpc1.7`只会保持一个TCP链接。当配置了多个集群端点时，客户端会向所有端点发起链接。一旦链接成立，均衡器就会保持该链接，并且关闭其他链接。直到一个来自服务器或客户端网络故障的错误被发送到客户端错误处理程序(见figure 3)。

![client-balancer-figure-02.png](https://cdn.jsdelivr.net/gh/CJH876492153/picture@main/client-balancer-figure-02.png)

![client-balancer-figure-03.png](https://cdn.jsdelivr.net/gh/CJH876492153/picture@main/client-balancer-figure-03.png)

客户端错误处理器接收到来自gRPC服务的错误后，基于错误码和错误信息决定是否重试或切换端点（见Figure 4 和 5）。

![client-balancer-figure-04.png](https://cdn.jsdelivr.net/gh/CJH876492153/picture@main/client-balancer-figure-04.png)

![client-balancer-figure-05.png](https://cdn.jsdelivr.net/gh/CJH876492153/picture@main/client-balancer-figure-05.png)

像 Watch 和 KeepAlive 这样的流 RPC 通常被请求时没有超时时间。相反，客户端可以定期发送 HTTP/2 ping 来检查固定端点的状态; 如果服务器没有响应，则均衡器切换到其他端点(参见*Figure 6*)。

![client-balancer-figure-06.png](https://cdn.jsdelivr.net/gh/CJH876492153/picture@main/client-balancer-figure-06.png)

### 均衡器局限性

`clientv3-grpc1.7`采用 HTTP/2 Ping 的形式观测端点是否存活，无法分辨集群角色，因此无法检测到网络分区。这就会导致请求卡死。理想情况下，keepalive ping应该在请求超时之前检测到网络分区并且做出端点切换。

![client-balancer-figure-07.png](https://cdn.jsdelivr.net/gh/CJH876492153/picture@main/client-balancer-figure-07.png)

`clientv3-grpc1.7`均衡器保存着一个不健康端点列表。断链地址会被添加到该列中，并且在5s内都认为是不可用状态。

![client-balancer-figure-08.png](https://cdn.jsdelivr.net/gh/CJH876492153/picture@main/client-balancer-figure-08.png)

## clientv3-grpc1.23

### 均衡器概览

`Clientv3-grpc1.23`的主要目标是简化均衡器故障转移逻辑; 而不是维护一个可能过时的不健康端点列表，只需在客户端与当前端点断开连接时循环到下一个端点。它不假定端点状态。因此，不需要更复杂的状态跟踪(参见图8和以上)。升级到 clientv3-grpc1.23应该不成问题; 所有更改都是内部的，同时保持所有向后兼容性。

在内部，当给定多个端点时，clientv3-grpc1.23 会创建多个子连接（每个端点一个子连接），而 clientv3-grpc1.7 仅创建一个到固定端点的连接（参见图 9）。例如，在 5 节点集群中，`clientv3-grpc1.23` 均衡器需要 5 个 TCP 连接，而 `clientv3-grpc1.7` 只需要 1 个。通过保留 TCP 连接池，`clientv3-grpc1.23` 可能会消耗更多资源，但提供更灵活的负载均衡器和更好的故障转移性能。默认的平衡策略是轮询，但可以轻松扩展以支持其他类型的均衡器（例如，二次选择、领导者选举等）。 `clientv3-grpc1.23`使用gRPC解析器组并实现均衡器选择器策略，以便将复杂的平衡工作委托给上游gRPC。另一方面，`clientv3-grpc1.7` 手动处理每个 gRPC 连接和均衡器故障转移，这使实现变得复杂。 `clientv3-grpc1.23` 在 gRPC 拦截器链中实现重试，自动处理 gRPC 内部错误并启用退避等更高级的重试策略，而 `clientv3-grpc1.7` 则手动解释 gRPC 错误以进行重试。

![client-balancer-figure-09.png](https://cdn.jsdelivr.net/gh/CJH876492153/picture@main/client-balancer-figure-09-20230626164345255.png)

### 均衡器局限性

可以通过缓存每个端点的状态来进行改进。例如，balancer 可以提前 ping 每个服务器以维护一个健康候选服务器列表，并在进行循环时使用这些信息。或者当断开连接时，平衡器可以优先考虑健康的端点。这可能会使平衡器的实现复杂化，因此可以在以后的版本中解决。

客户端 keepalive ping 仍然无法推断网络分区。 流请求可能会被分区节点卡住。 需要实现高级健康检查服务来了解集群成员状态。

![client-balancer-figure-07.png](https://cdn.jsdelivr.net/gh/CJH876492153/picture@main/client-balancer-figure-07-20230626170135166.png)
